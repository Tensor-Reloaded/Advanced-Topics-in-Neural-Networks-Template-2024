## Lab 11


***
References:
* Generalization
    * [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836)
    * [Train longer, generalize better: closing the generalization gap in large batch training of neural networks](https://arxiv.org/abs/1705.08741)
    * [Sharpness-Aware Minimization for Efficiently Improving Generalization](https://arxiv.org/abs/2010.01412)
    * [SAM Github](https://github.com/davda54/sam)
* Benchmarks:
    * CIFAR-100: https://paperswithcode.com/sota/image-classification-on-cifar-100
    * CIFAR-100-Noisy: https://paperswithcode.com/sota/learning-with-noisy-labels-on-cifar-100n
    * [94% on CIFAR-10 in 3.29 Seconds on a Single GPU](https://arxiv.org/abs/2404.00498)
* Mamba:
    * [Github](https://github.com/state-spaces/mamba)
    * https://arxiv.org/abs/2405.21060
* xLSTM:
    * [Github](https://github.com/NX-AI/xlstm)
    * https://arxiv.org/abs/2405.04517
