## Lab 9

***
### Project: [ATNN_2024_Assignment_4.pdf](./ATNN_2024_Assignment_4.pdf)

***
For self-study (all students):
* Annotated deep learning papers: https://github.com/labmlai/annotated_deep_learning_paper_implementations
* LSTM: 
    * intro: https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm/
    * code: https://nn.labml.ai/lstm/index.html
* RNN vs GRU vs LSTM: https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573
* Multi-Headed Attention [implementation](https://nn.labml.ai/transformers/mha.html) from Attention Is All You Need. 

Advanced (for students who want to learn more):
- LSTM paper: https://deeplearning.cs.cmu.edu/F23/document/readings/LSTM.pdf
- Attention Is All You Need: https://arxiv.org/abs/1706.03762
- Attention Is ALl You Need video: https://www.youtube.com/watch?v=iDulhoQ2pro


***
References:
- RNN tutorial: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html
- https://jalammar.github.io/illustrated-transformer/
- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
