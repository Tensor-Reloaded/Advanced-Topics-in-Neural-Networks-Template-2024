## Lab 10


***
For self-study (all students):
- Attention Is All You Need: https://arxiv.org/abs/1706.03762
- Attention Is ALl You Need video: https://www.youtube.com/watch?v=iDulhoQ2pro
 - Must watch: [Neural Networks (chapter 5 - chapter 7)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

Supplementary materials:
- Multi-Headed Attention [implementation](https://nn.labml.ai/transformers/mha.html). 
- Transformer Encoder and Decoder Models [implementation](https://nn.labml.ai/transformers/models.html)
- Fixed Positional Encodings [implementation](https://nn.labml.ai/transformers/positional_encoding.html)

Advanced (for students who want to learn more):
- An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT): [paper](https://arxiv.org/abs/2010.11929), [code](https://nn.labml.ai/transformers/vit/index.html), [video](https://www.youtube.com/watch?v=TrdevFK_am4)
- BERT: [paper](https://arxiv.org/abs/1810.04805) [code](https://nn.labml.ai/transformers/mlm/index.html), [video](https://www.youtube.com/watch?v=-9evrZnBorM)
- GPT-2: [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), [code](https://nn.labml.ai/transformers/gpt/index.html), [video](https://www.youtube.com/watch?v=u1_qMdb0kYU)

*** 

Transformer and ViT implementations:
* https://github.com/lucidrains/vit-pytorch
* https://github.com/lucidrains/x-transformers

***
References:
- https://jalammar.github.io/illustrated-transformer/
- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

