{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkf0tLx7j8nf",
        "outputId": "5f1a4c92-a0cb-46f0-90f1-47bcc108f754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 517.6520, Train Accuracy: 0.8255\n",
            "Epoch 2, Train Loss: 311.6589, Train Accuracy: 0.8999\n",
            "Epoch 3, Train Loss: 259.6775, Train Accuracy: 0.9176\n",
            "Epoch 4, Train Loss: 228.4081, Train Accuracy: 0.9257\n",
            "Epoch 5, Train Loss: 210.0848, Train Accuracy: 0.9324\n",
            "Epoch 6, Train Loss: 195.7329, Train Accuracy: 0.9369\n",
            "Epoch 7, Train Loss: 184.5106, Train Accuracy: 0.9413\n",
            "Epoch 8, Train Loss: 179.4707, Train Accuracy: 0.9420\n",
            "Epoch 9, Train Loss: 169.1816, Train Accuracy: 0.9444\n",
            "Epoch 10, Train Loss: 161.8299, Train Accuracy: 0.9470\n",
            "Epoch 11, Train Loss: 157.5534, Train Accuracy: 0.9487\n",
            "Epoch 12, Train Loss: 153.7473, Train Accuracy: 0.9496\n",
            "Epoch 13, Train Loss: 148.8627, Train Accuracy: 0.9530\n",
            "Epoch 14, Train Loss: 145.2421, Train Accuracy: 0.9526\n",
            "Epoch 15, Train Loss: 144.4509, Train Accuracy: 0.9525\n",
            "Epoch 16, Train Loss: 138.2497, Train Accuracy: 0.9548\n",
            "Epoch 17, Train Loss: 138.9159, Train Accuracy: 0.9554\n",
            "Epoch 18, Train Loss: 134.0489, Train Accuracy: 0.9563\n",
            "Epoch 19, Train Loss: 133.4787, Train Accuracy: 0.9567\n",
            "Epoch 20, Train Loss: 129.8441, Train Accuracy: 0.9577\n",
            "Epoch 21, Train Loss: 131.0581, Train Accuracy: 0.9566\n",
            "Epoch 22, Train Loss: 127.7168, Train Accuracy: 0.9585\n",
            "Epoch 23, Train Loss: 126.1812, Train Accuracy: 0.9590\n",
            "Epoch 24, Train Loss: 125.7748, Train Accuracy: 0.9586\n",
            "Epoch 25, Train Loss: 124.0731, Train Accuracy: 0.9598\n",
            "Epoch 26, Train Loss: 121.5165, Train Accuracy: 0.9594\n",
            "Epoch 27, Train Loss: 120.0784, Train Accuracy: 0.9611\n",
            "Epoch 28, Train Loss: 119.2390, Train Accuracy: 0.9603\n",
            "Epoch 29, Train Loss: 117.4053, Train Accuracy: 0.9610\n",
            "Epoch 30, Train Loss: 115.8089, Train Accuracy: 0.9619\n",
            "Validation Loss: 0.2729, Validation Accuracy: 0.9553\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data = datasets.MNIST(root=\"data\", train=True, transform=ToTensor(), download=True)\n",
        "test_data = datasets.MNIST(root=\"data\", train=False, transform=ToTensor(), download=True)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.2):\n",
        "        self.W1 = torch.randn(input_size, hidden_size, device=device) * math.sqrt(2.0 / input_size)\n",
        "        self.b1 = torch.zeros(hidden_size, device=device)\n",
        "        self.W2 = torch.randn(hidden_size, output_size, device=device) * math.sqrt(2.0 / hidden_size)\n",
        "        self.b2 = torch.zeros(output_size, device=device)\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def relu(self, x):\n",
        "        return torch.maximum(x, torch.tensor(0.0, device=device))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
        "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n",
        "\n",
        "    def apply_dropout(self, x):\n",
        "        dropout_mask = (torch.rand(x.shape, device=device) > self.dropout_prob).float()\n",
        "        return x * dropout_mask / (1 - self.dropout_prob)\n",
        "\n",
        "    def forward(self, X, train=True):\n",
        "        self.Z1 = X @ self.W1 + self.b1\n",
        "        self.A1 = self.relu(self.Z1)\n",
        "        if train:\n",
        "            self.A1 = self.apply_dropout(self.A1)\n",
        "        self.Z2 = self.A1 @ self.W2 + self.b2\n",
        "        self.A2 = self.softmax(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def cross_entropy_loss(self, predictions, targets):\n",
        "        epsilon = 1e-10\n",
        "        batch_size = predictions.shape[0]\n",
        "        log_likelihood = -torch.log(predictions[range(batch_size), targets] + epsilon)\n",
        "        loss = torch.sum(log_likelihood) / batch_size\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, Y, learning_rate, weight_decay=4e-4):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Gradient for output layer\n",
        "        dZ2 = self.A2.clone()\n",
        "        dZ2[range(m), Y] -= 1\n",
        "        dW2 = (self.A1.T @ dZ2) / m + weight_decay * self.W2\n",
        "        db2 = dZ2.sum(axis=0) / m\n",
        "\n",
        "        # Gradient for hidden layer\n",
        "        dA1 = dZ2 @ self.W2.T\n",
        "        dZ1 = dA1 * (self.Z1 > 0).float()  # Derivative of ReLU\n",
        "        dW1 = (X.T @ dZ1) / m + weight_decay * self.W1\n",
        "        db1 = dZ1.sum(axis=0) / m\n",
        "\n",
        "        # Update weights and biases manually\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "\n",
        "def learning_rate_decay(epoch, initial_learning_rate, decay_factor=0.95):\n",
        "    return initial_learning_rate * (decay_factor ** (epoch // 5))\n",
        "\n",
        "def train(model, train_loader, initial_learning_rate=0.03, epochs=30):\n",
        "    for epoch in range(epochs):\n",
        "        learning_rate = learning_rate_decay(epoch, initial_learning_rate)\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        for X, Y in train_loader:\n",
        "            X, Y = X.view(-1, 28*28).to(device), Y.to(device)\n",
        "            X = (X - 0.5) / 0.5  # Normalize input\n",
        "\n",
        "            predictions = model.forward(X, train=True)\n",
        "\n",
        "            loss = model.cross_entropy_loss(predictions, Y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            model.backward(X, Y, learning_rate)\n",
        "\n",
        "            predicted_labels = predictions.argmax(dim=1)\n",
        "            correct += (predicted_labels == Y).sum().item()\n",
        "\n",
        "        accuracy = correct / len(train_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Train Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "def validate(model, test_loader):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for X, Y in test_loader:\n",
        "        X, Y = X.view(-1, 28*28).to(device), Y.to(device)\n",
        "\n",
        "        predictions = model.forward(X, train=False)\n",
        "\n",
        "        loss = model.cross_entropy_loss(predictions, Y)\n",
        "        total_loss += loss.item()\n",
        "        predicted_labels = predictions.argmax(dim=1)\n",
        "        correct += (predicted_labels == Y).sum().item()\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "    return total_loss / len(test_loader), accuracy\n",
        "\n",
        "model = MLP(784, 100, 10, dropout_prob=0.15)\n",
        "train(model, train_loader, initial_learning_rate=0.03, epochs=30)\n",
        "validation_loss, validation_accuracy = validate(model, test_loader)\n",
        "print(f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
